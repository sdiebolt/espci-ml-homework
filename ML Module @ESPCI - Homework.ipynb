{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This cell declares LaTeX macros.*\n",
    "$\\DeclareMathOperator*{argmax}{arg\\,max}$\n",
    "$\\DeclareMathOperator{SE}{SE}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Course Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Maximum Likelihood Estimation\n",
    "\n",
    "*Alice throws a coin 100 times and obtains 55 times a tail. Estimate by maximum likelihood the probability that the coin gives a tails. What confidence do we have in this result? Should Alice consider the coin to be unfair?*\n",
    "\n",
    "We're modelling the outcome of a coin flip by a Bernoulli distribution \n",
    "\n",
    "\\begin{equation}\n",
    "    p_{\\text{model}}(x; \\theta) = \\theta^x (1 - \\theta)^{1 - x},\n",
    "\\end{equation} \n",
    "\n",
    "where the parameter $\\theta$ represents the probability of getting a tails. Thus, the maximum likelihood estimator for $\\theta$ is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\theta_{\\text{ML}} &= \\argmax_\\theta \\mathbb{E}_{x \\sim \\widehat{p}_{\\text{data}}} [\\log p_{\\text{model}}(x; \\theta)]\\\\\n",
    "        &= \\argmax_\\theta \\left(\\widehat{\\theta} \\log\\theta + (1 - \\widehat{\\theta}) \\log(1 - \\theta)\\right)\\\\\n",
    "        &= \\widehat{\\theta},\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\widehat{\\theta}$ is the sample estimate of the probability that the coin gives a tails and $\\widehat{p}_{\\text{data}}$ is the empirical distribution defined by the observed sample.\n",
    "\n",
    "Given that after 100 throws, Alice obtained 55 tails, then\n",
    "\n",
    "\\begin{equation}\n",
    "    \\theta_{\\text{ML}} = \\widehat{\\theta} = 0.55.\n",
    "\\end{equation}\n",
    "\n",
    "Knowing that \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\SE(\\theta_{\\text{ML}}) &= \\sqrt{\\frac{\\widehat{\\theta} (1 - \\widehat{\\theta})}{n}}\\\\\n",
    "        &\\approx 0.05\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "and considering the normal assumption ($100 \\times 0.55 > 30$), the 95% confidence interval is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\theta_{\\text{ML}} \\in \\theta_{\\text{ML}} \\pm 1.96\\times \\SE(\\theta_{\\text{ML}}) \\approx [0.45, 0.65].\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, as the 95% confidence interval contains 0.5, Alice cannot reject the null hypothesis that her coin is fair (at the 5% confidence level)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Bayesian Estimation\n",
    "\n",
    "*Bob is tested for a disease. The test, which is either positive or negative, is only 90% reliable. Given that 1% of people of Bob’s age and background have the disease, what is the probability that Bob has the disease?*\n",
    "\n",
    "Since we know nothing about the test results, we will assume that the test came back positive (without this assumption, the only valid conclusion would be that Bob has a 1% probability of having the disease).\n",
    "\n",
    "Considering the events \n",
    "\n",
    "- **ill**: \"Bob has the disease\",\n",
    "- **+**: \"Bob has tested positive\",\n",
    "\n",
    "and given that \n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        P(+ | \\text{ill}) = 0.90,\\\\\n",
    "        P(\\text{ill}) = 0.01,\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "then using Bayes' theorem,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        P(\\text{ill} | +) &= \\frac{P(\\text{ill}, +)}{P(+)}\\\\\n",
    "        &= \\frac{P(+ | \\text{ill}) P(\\text{ill})}{P(+ | \\text{ill})P(\\text{ill}) + P(+ | \\overline{\\text{ill}})P(\\overline{\\text{ill}})}\\\\\n",
    "        &= \\frac{0.90 \\times 0.01}{0.90 \\times 0.01 + 0.10 \\times 0.99}\\\\\n",
    "        &\\approx 0.08.\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, given that Bob has tested positive, he has a probability of about 8% to have the disease. This low probability reflects the lack of information we have (in fact, in a medical environment, the test result would be called *reactive* instead of *positive* and further testing would be done). Obvisouly, other factors (contact with contagious people, etc.), if taken into account in a refined model, could help Bob know if the result was a false or a true positive.\n",
    "\n",
    "For example, a second positive test, given that the prior $P(\\text{ill})$ has been updated to 8%, would give a probability of about 44% that Bob has the disease. A third positive test would increase this probability to 88%.\n",
    "\n",
    "*By redesigning the test, you can either reduce from 10% to 5% the false positive rate (less negative results when the patient is positive) or reduce from 10% to 5% the false negative rate (less positive results when the patient is negative): what is preferable?*\n",
    "\n",
    "In the field of disease testing, false negatives are usually considered worse than false positives. Taking COVID-19 as an example: a false positive would lead to a heavy mental burden for the patient, but further testing would eventually show that they aren't ill, while a false negative would mean that the patient would be missing out on crucial treatments and runs a risk of spreading the disease. Therefore, decreasing the false negative rate is preferable for this test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Information Theory\n",
    "\n",
    "*The binary erasure channel is a discrete memoryless channel where each input $x_i \\in \\{0, 1\\}$ is either transmitted reliably, with probability $1−\\epsilon$, or replaced by an error symbol $\\ast$, with probability $\\epsilon$. What is the capacity of this channel?*\n",
    "\n",
    "For a binary erasure channel with a probability $\\epsilon$ of transmitting an error,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        I(X; Y) &= H(X) - H(X | Y)\\\\\n",
    "        &= H(X) - P(y = 0) H(X | y = 0) - P(y = 1) H(X | y = 1) - P(y = \\ast) H(X | y = \\ast).\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Given $y \\neq \\ast$, $X$ is entirely determined by $Y$, thus \n",
    "\n",
    "\\begin{equation}\n",
    "    H(X | y = 0) = H(X | y = 1) = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Also, \n",
    "\n",
    "\\begin{equation}\n",
    "    P(y = \\ast) = \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "by definition of a binary erasure channel, and \n",
    "\n",
    "\\begin{equation}\n",
    "    H(X |y = \\ast) = H(X)\n",
    "\\end{equation}\n",
    "\n",
    "by symmetry (knowing that $y = \\ast$ doesn't give any information about $X$, so $P(X | y = \\ast) = P(X)$). Therefore,\n",
    "\n",
    "\\begin{equation}\n",
    "    I(X; Y) = (1 - \\epsilon)H(X).\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the capacity of a binary erasure channel is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        C &= \\sup_{p_X} I(X; Y)\\\\\n",
    "        &= (1 - \\epsilon) \\sup_{p_X} H(X)\\\\\n",
    "        &= 1 - \\epsilon.\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "The last equation holds if $X$ comes from a Bernoulli distribution with parameter $\\frac{1}{2}$, as the symmetry would suggest.\n",
    "\n",
    "*More generally, a memoryless erasure channel takes inputs from an alphabet of $q$ symbols $\\{1, 2, \\dots, q\\}$: any of these symbols is transmitted reliably with probability $1 − \\epsilon$ and replaced by an error symbol $\\ast$ with probability $\\epsilon$. What is the capacity of this channel?*\n",
    "\n",
    "A memoryless erasure channel taking inputs from an alphabet of $q$ symbols would have the same capacity as a binary erasure channel. This result is analogous to the previous proof, where $X$ now comes from a categorical distribution (generalized Bernoulli) with parameter $\\frac{1}{q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Maximum Entropy Method\n",
    "\n",
    "*Consider $N$ binary sequences of length $p$: $\\sigma_{i, j} = \\pm 1$ with $i = 1, \\dots, N$ and $j = 1, \\dots , p$. We use the maximum entropy method to estimate $P(\\sigma_1, \\dots, \\sigma_p)$. Show that if we choose to constrain for each $j$ the average of $\\sigma_j$ to the empirical mean $\\mu_j = 􏰖\\sum_i \\frac{\\sigma_{i, j}}{N}$ , the maximum entropy principle leads to a distribution of the form*\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\sigma_1, \\dots, \\sigma_p) = \\frac{e^{\\sum_j h_j \\sigma_j}}{Z}.\n",
    "\\end{equation}\n",
    "\n",
    "First, we know that for a given $j$,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        \\langle \\sigma_j \\rangle = \\sum_{\\sigma_j} P(\\sigma_j) \\sigma_j,\\\\\n",
    "        \\mu_j \\equiv \\overline{\\sigma_j} = \\frac{1}{N} \\sum_{i = 1}^N \\sigma_{ij}.\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "We are trying to estimate the probability distribution under the constraint\n",
    "\n",
    "\\begin{equation}\n",
    "    \\langle \\sigma_j \\rangle = \\mu_j,\n",
    "\\end{equation}\n",
    "\n",
    "knowing that it must follow the normalization constraint \n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{\\sigma_j} P(\\sigma_j) = 1.\n",
    "\\end{equation}\n",
    "\n",
    "By using the MaxEnt method, we require the probability distribution to maximize the information entropy\n",
    "\n",
    "\\begin{equation}\n",
    "    S = -\\sum_{\\sigma_j} P(\\sigma_j) \\log P(\\sigma_j).\n",
    "\\end{equation}\n",
    "\n",
    "The corresponding Lagrangian $\\mathcal{L} = \\mathcal{L}(P(\\sigma_j); \\lambda_1, \\lambda_2)$ has the functional form\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L} = S - \\lambda_1 \\left(\\sum_{\\sigma_j} P(\\sigma_j) - 1\\right) - \\lambda_2 \\left(\\sum_{\\sigma_j} P(\\sigma_j) \\sigma_j - \\mu_j\\right)\n",
    "\\end{equation}\n",
    "\n",
    "and its stationary point is found at\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial P(\\sigma_j)} = 0\\\\\n",
    "        \\iff& -\\log P(\\sigma_j) - 1 + \\lambda_1 + \\lambda_2 \\sigma_j = 0\\\\\n",
    "        \\iff& P(\\sigma_j) = \\frac{1}{Z_j} e^{\\lambda_2 \\sigma_j},\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where $Z_j = e^{1 - \\lambda_1}$. By reintroducing the constraints, we get\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        \\sum_{\\sigma_j} P(\\sigma_j) = \\frac{1}{Z_j} (e^{\\lambda_2} + e^{-\\lambda_2}) = 1, &(1)\\\\\n",
    "        \\sum_{\\sigma_j} P(\\sigma_j) \\sigma_j = \\frac{1}{Z_j} (e^{\\lambda_2} - e^{-\\lambda_2}) = \\mu_j. &(2)\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\frac{(2)}{(1)} &\\iff \\frac{e^{\\lambda_2} - e^{-\\lambda_2}}{e^{\\lambda_2} + e^{-\\lambda_2}} = \\mu_j\\\\\n",
    "        &\\iff \\lambda_2 = \\frac{1}{2} \\log\\frac{1 + \\mu_j}{1 - \\mu_j},\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        (1) + (2) &\\iff \\frac{2}{Z_j} e^{\\lambda_2} = 1 + \\mu_j\\\\\n",
    "        &\\iff Z_j = \\frac{2}{1 + \\mu_j} \\times \\sqrt{\\frac{1 + \\mu_j}{1 - \\mu_j}}\\\\\n",
    "        &\\iff Z_j = \\frac{2}{\\sqrt{1 - \\mu_j^2}}.\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, considering that the $\\sigma_j$ are independent,\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\sigma_1, \\dots, \\sigma_p) = \\prod_{j = 1}^p P(\\sigma_j) = \\frac{e^{\\sum_j h_j \\sigma_j}}{Z},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        h_j = \\frac{1}{2} \\log\\frac{1 + \\mu_j}{1 - \\mu_j},\\\\\n",
    "        Z = \\frac{2^p}{\\prod_j \\sqrt{1 - \\mu_j^2}}.\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "*What are the values of $h_j$ and $Z$? What if we take $\\sigma_{i, j} \\in \\{0, 1\\}$ instead of $\\sigma_{i, j} \\in \\{−1, 1\\}$?*\n",
    "\n",
    "If $\\sigma_{i, j} \\in \\{0, 1\\}$, then equations $(1)$ and $(2)$ become\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        \\sum_{\\sigma_j} P(\\sigma_j) = \\frac{1}{Z_j}(1 + e^{\\lambda_2}) = 1, &(1)\\\\\n",
    "        \\sum_{\\sigma_j} P(\\sigma_j) \\sigma_j = \\frac{1}{Z_j} e^{\\lambda_2} = \\mu_j, &(2)\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "leading to a Bernoulli probability distribution for $\\sigma_j$:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\sigma_j) = \\mu_j^{\\sigma_j} (1 - \\mu_j)^{1 - \\sigma_j}.\n",
    "\\end{equation}\n",
    "\n",
    "Finally,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        &P(\\sigma_1, \\dots, \\sigma_p) = \\prod_{j = 1}^p \\mu_j^{\\sigma_j} (1 - \\mu_j)^{1 - \\sigma_j}\\\\\n",
    "        \\iff& \\log P(\\sigma_1, \\dots, \\sigma_p) = \\sum_{j = 1}^p \\log \\left(\\mu_j^{\\sigma_j} (1 - \\mu_j)^{1 - \\sigma_j}\\right)\\\\\n",
    "        \\iff& P(\\sigma_1, \\dots, \\sigma_p) = \\frac{e^{\\sum_{j = 1}^p \\log\\left(\\frac{\\mu_j}{1 - \\mu_j}\\right) \\sigma_j}}{e^{-\\sum_{j = 1}^p \\log(1 - \\mu_j)}}.\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore $h_j$ and $Z$ are\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        h_j = \\log\\left(\\frac{\\mu_j}{1 - \\mu_j}\\right),\\\\\n",
    "        Z = e^{-\\sum_{j = 1}^p \\log(1 - \\mu_j)}.\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Lasso Regression And Model Selection: What Makes A Good Wine?\n",
    "\n",
    "This section will study the qualities of *vinho verde*, a wine from the Minho (northwest) region of Portugal. The provided datasets (Cortez *et al.*, 2009) are available at http://www3.dsi.uminho.pt/pcortez/wine/ and contain twelve features of a sample of red and white *vinho verde* wines, ranging from objective test results (e.g. pH values) to sensory data (evaluations made by wine experts).\n",
    "\n",
    "The goal of this section is to preprocess and use these datasets to\n",
    "\n",
    "- find a predictive model of red wine quality using lasso regression;\n",
    "- implement a classifier that will be able to recognize red and white wines from their physicochemical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries.\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import lasso_path, Lasso, LassoCV\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set random generator seed.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Preprocessing\n",
    "\n",
    "The different features available in the `winequality-red` dataset are displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read red wine quality dataset and display table head.\n",
    "red_wine = pd.read_csv(\"../input/winequality/winequality-red.csv\", sep=\";\")\n",
    "red_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms of these features are displayed below. Some features, like `residual sugars` and `chlorides`, show a number of outliers that could introduce bias in the following analyses. Therefore, a preprocessing step will remove values outside the ±3 standard deviations range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each variable.\n",
    "fig = make_subplots(rows=4, cols=3)\n",
    "\n",
    "for i, col in enumerate(red_wine.columns, 1):\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=red_wine[col],\n",
    "            name=col\n",
    "        ), \n",
    "        row=int(np.ceil(i / 3)), \n",
    "        col=i % 3 if i % 3 != 0 else 3\n",
    "    )\n",
    "    \n",
    "fig.update_layout(\n",
    "    title_text=\"Histograms Of Red Wine Features – Before Outlier Removal\",\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from all features (outside the +-3 std range).\n",
    "red_wine_clean = red_wine[(np.abs(stats.zscore(red_wine)) < 3).all(axis=1)]\n",
    "\n",
    "# Plot histograms for each variable.\n",
    "fig = make_subplots(rows=4, cols=3)\n",
    "\n",
    "for i, col in enumerate(red_wine_clean.columns, 1):\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=red_wine_clean[col],\n",
    "            name=col\n",
    "        ), \n",
    "        row=int(np.ceil(i / 3)), \n",
    "        col=i % 3 if i % 3 != 0 else 3\n",
    "    )\n",
    "    \n",
    "fig.update_layout(\n",
    "    title_text=\"Histograms Of Red Wine Features – After Outlier Removal\",\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Principal Component Analysis\n",
    "\n",
    "PCA is performed to visualize correlations between the different features in the `winequality-red` dataset. All features are scaled to zero mean and unit variance beforehand (otherwise PCA might determine that the direction of maximal variance corresponds to features varying more than others because of their scales).\n",
    "\n",
    "Two plots are used to study correlations:\n",
    "\n",
    "- a scree plot, showing eigenvalues ordered from largest to smallest. This plot can be used to determine the number of eigenvalues to keep for a later analysis;\n",
    "- a variables factor map, showing the correlations between all twelve features and the top two principal components. Features that are grouped together are positively correlated, while features on opposite sides of the unit circle are negatively correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned dataset is scaled to zero mean and unit variance.\n",
    "red_wine_norm = pd.DataFrame(\n",
    "    StandardScaler().fit_transform(red_wine_clean),\n",
    "    columns=red_wine_clean.columns\n",
    ")\n",
    "\n",
    "# Fit PCA model.\n",
    "pca_model = PCA()\n",
    "pca_result = pca_model.fit_transform(red_wine_norm)\n",
    "\n",
    "# Get correlations between components and features.\n",
    "coef = pd.DataFrame(pca_model.components_.T[:, 0:2],\n",
    "                    columns=[\"pc1\", \"pc2\"],\n",
    "                    index = red_wine.columns)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    subplot_titles=(\"Scree Plot\", \"Variables Factor Map\"))\n",
    "\n",
    "# Scree plot.\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[\"PC {}\".format(i) for i in range(1, pca_model.components_.shape[1]+1)],\n",
    "        y=pca_model.explained_variance_ratio_,\n",
    "        name=\"Scree Plot\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Variables factor map.\n",
    "# The angle in the unit circle is used to compute the point color. This way\n",
    "# correlated features will appear with similar colors.\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=coef.pc1,\n",
    "        y=coef.pc2,\n",
    "        text=red_wine.columns,\n",
    "        mode=\"markers+text\",\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=np.angle(coef.pc1 + 1j*coef.pc2, deg=True)\n",
    "        ),\n",
    "        name=\"Variables Factor Map\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Add unit circle.\n",
    "fig.update_layout(\n",
    "    shapes=[\n",
    "        # unfilled circle\n",
    "        dict(\n",
    "            type=\"circle\",\n",
    "            xref=\"x2\",\n",
    "            yref=\"y2\",\n",
    "            x0=-1,\n",
    "            y0=-1,\n",
    "            x1=1,\n",
    "            y1=1\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "    \n",
    "# Update figure size, axes titles and remove legend.\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    width=1200, \n",
    "    showlegend=False,\n",
    "    xaxis_title=\"Component Number\",\n",
    "    yaxis_title=\"Explained Variance Ratio\",\n",
    "    xaxis2_title=\"PCA 1 ({:.2f}%)\".format(pca_model.explained_variance_ratio_[0]*100),\n",
    "    yaxis2_title=\"PCA 2 ({:.2f}%)\".format(pca_model.explained_variance_ratio_[1]*100)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scree plot shows that the top two principal components explain about 45% of the variance. Therefore, using only the top two components to analyse the data is risky, as a lot of information is lost in the higher dimensions. \n",
    "\n",
    "On the variables factor map, using the top two components, we observe:\n",
    "\n",
    "- a strong positive correlation between the `alcohol` and `quality` features, with a slightly weaker correlation between these features and the `sulphates`.\n",
    "- a strong negative correlation between the `alcohol`, `quality` and `sulphates` features and the `volatile acidity`, `total sulfur dioxide` and `free sulfur dioxide` features.\n",
    "\n",
    "As previously stated, considering that the rest of the components still explain a significant part of the variance, the variables factor map should be interpreted with caution: in higher dimensions, some of these points could be farther apart than seen on a 2D visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Multivariate Linear Regression\n",
    "\n",
    "We perform multivariate linear regression (ordinary least squares) of the `quality` score against the remaining 11 input features (with intercept). The estimated model parameters and the coefficient of determination are compared when the features are standardized either by simply removing the mean, or by both removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize input features (zero mean, with or without unit variance).\n",
    "X_mean_removed = StandardScaler(with_std=False).fit_transform(red_wine_clean.drop(\"quality\", axis=1))\n",
    "X_norm = StandardScaler().fit_transform(red_wine_clean.drop(\"quality\", axis=1))\n",
    "\n",
    "y = red_wine_clean.quality\n",
    "\n",
    "# Perform multivariate linear regression with both standardization conditions.\n",
    "# A constant column is added to the input features for the intercept term.\n",
    "est_zero_mean = sm.OLS(y, sm.add_constant(X_mean_removed)).fit()\n",
    "est_norm = sm.OLS(y, sm.add_constant(X_norm)).fit()\n",
    "\n",
    "# Plot regression coefficients as grouped bar plots.\n",
    "# Significant coefficients are colored in red, non-significant in gray.\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1, \n",
    "    shared_xaxes=True, \n",
    "    vertical_spacing=0.1,\n",
    "    subplot_titles=(\"Zero Mean\", \"Zero Mean, Unit Variance\")\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=red_wine.columns,\n",
    "        y=est_zero_mean.params[1:],\n",
    "        name=\"Zero Mean\",\n",
    "        marker_color=[\"crimson\" if p <= 0.05 else \"lightslategray\" for p in est_zero_mean.pvalues[1:]]\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=red_wine.columns,\n",
    "        y=est_norm.params[1:],\n",
    "        name=\"Zero Mean, Unit Variance\",\n",
    "        marker_color=[\"crimson\" if p <= 0.05 else \"lightslategray\" for p in est_norm.pvalues[1:]]\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Multivariate Linear Regression Coefficients, R² = {:.2f}, red: p < 0.05 \".format(est_norm.rsquared),\n",
    "    showlegend=False,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept term isn't displayed as it would hinder the visualization (it correponds to the mean of the `quality` score). As expected, the standard scaling of the dataset prior to regression (zero mean and unit variance) is essential to interpret the results: when the individual features aren't comparable due to their difference in scales, their respective regression coefficients aren't comparable either.\n",
    "\n",
    "The coefficient of determination doesn't change between both settings, as it is already normalized (normalized covariance). Its value is relatively low: the current model including all input features and an intercept term explains only about 37% of the variance of the `quality` feature. Looking back at the histogram of the `quality` feature, we can see that it has very low variance, with most scores being either 5 or 6. Seeing that the difference in `quality` between the wines is almost binary, the multivariate linear model isn't well suited. To obtain better predictions, further processing of the `quality` feature would be necessary (e.g. using a continuous scale), or a new model could be used (e.g. multivariate logistic regression, by binarizing the `quality` score).\n",
    "\n",
    "Most regression coefficients aren't significant at the 5% level: the significant features are colored in red on the above box plots. From the remaining features, we observe that\n",
    "\n",
    "- the `alcohol` and `sulphates` features show positive influence on the `quality` score;\n",
    "- the `volatile acidity`, `total sulfur dioxide` and `pH` features show negative influence on the `quality` score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Lasso Regression And Model Selection\n",
    "\n",
    "As the OLS model including all input features was inaccurate, lasso is suggested in this section as a way to select relevant features for the prediction of `quality`. However, lasso is likely to suffer the same fate as OLS, as it won't be able to explain more variance than the full OLS model.\n",
    "\n",
    "The lasso model will be fitted along a given regularization path: $\\alpha\\in[10^{-3}, 1]$. To assess performance, the cleaned dataset will be split into a training set of size 500 and a testing test of size 941. A scaler for zero mean and unit variance will be fit on the training set and the transformation applied on both training and testing sets. Finally, to find the best model, 10-fold cross-validation will be used along the regularization path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into testing and training set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    red_wine_clean.drop(\"quality\", axis=1), \n",
    "    red_wine_clean.quality, \n",
    "    train_size=500, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standardize data, fitting only on training set.\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "_, n = X_train.shape\n",
    "\n",
    "# Regularization parameters path.\n",
    "alphas = np.logspace(-3, 0, 100)\n",
    "\n",
    "# Compute lasso path with coordinate descent.\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X_train, y_train, alphas=alphas, random_state=42)\n",
    "\n",
    "# Compute lasso with iterative fitting along the regularization path.\n",
    "# The best model is chosen by 10-fold cross-validation.\n",
    "model = LassoCV(cv=10, alphas=alphas, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# Compute training and testing score (R^2) along the regularization path.\n",
    "training_score = []\n",
    "testing_score = []\n",
    "for a in alphas:\n",
    "    reg = Lasso(alpha=a, random_state=42).fit(X_train, y_train)\n",
    "    training_score.append(reg.score(X_train, y_train))\n",
    "    testing_score.append(reg.score(X_test, y_test))\n",
    "\n",
    "# Plot lasso coefficients, MSE and performance along the regularization path.\n",
    "fig = make_subplots(\n",
    "    rows=3, \n",
    "    cols=1, \n",
    "    vertical_spacing=0.1,\n",
    "    subplot_titles=(\"Lasso Path\", \"K-Fold Cross-Validation, K = 10\", \"Lasso Performance\")\n",
    ")\n",
    "\n",
    "# Lasso coefficients.\n",
    "for i in range(n):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=alphas_lasso,\n",
    "            y=coefs_lasso[i],\n",
    "            name=red_wine_clean.columns[i],\n",
    "            mode=\"lines\",\n",
    "            legendgroup=\"lasso\"\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "# Lasso MSE.\n",
    "for i in range(10):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=model.alphas_,\n",
    "            y=model.mse_path_[:, i],\n",
    "            mode=\"lines\",\n",
    "            line=dict(dash=\"dot\"),\n",
    "            marker_color=\"grey\",\n",
    "            showlegend=False,\n",
    "            name=\"MSE {}\".format(i)\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1\n",
    "    )\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=model.alphas_,\n",
    "        y=model.mse_path_.mean(axis=1),\n",
    "        mode=\"lines\",\n",
    "        marker_color=\"black\",\n",
    "        legendgroup=\"cv\",\n",
    "        name=\"Average MSE\"\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Lasso performance.\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=alphas,\n",
    "        y=training_score,\n",
    "        mode=\"lines\",\n",
    "        legendgroup=\"performance\",\n",
    "        name=\"Training\"\n",
    "    ),\n",
    "    row=3,\n",
    "    col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=alphas,\n",
    "        y=testing_score,\n",
    "        mode=\"lines\",\n",
    "        legendgroup=\"performance\",\n",
    "        name=\"Testing\"\n",
    "    ),\n",
    "    row=3,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add dashed line on each plot to indicate the regularization factor for the best model.\n",
    "min_alpha = model.alphas_[np.argmin(model.mse_path_.mean(axis=1))]\n",
    "fig.add_shape(\n",
    "    go.layout.Shape(\n",
    "        type=\"line\",\n",
    "        yref=\"y\",\n",
    "        xref=\"x\",\n",
    "        x0=min_alpha,\n",
    "        y0=-0.2,\n",
    "        x1=min_alpha,\n",
    "        y1=0.43,\n",
    "        line=dict(dash=\"dot\")\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "fig.add_shape(\n",
    "    go.layout.Shape(\n",
    "        type=\"line\",\n",
    "        yref=\"y\",\n",
    "        xref=\"x\",\n",
    "        x0=min_alpha,\n",
    "        y0=-0.2,\n",
    "        x1=min_alpha,\n",
    "        y1=0.43,\n",
    "        line=dict(dash=\"dot\")\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "fig.add_shape(\n",
    "    go.layout.Shape(\n",
    "        type=\"line\",\n",
    "        yref=\"y\",\n",
    "        xref=\"x\",\n",
    "        x0=min_alpha,\n",
    "        y0=0.27,\n",
    "        x1=min_alpha,\n",
    "        y1=0.9,\n",
    "        line=dict(dash=\"dot\")\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "fig.add_shape(\n",
    "    go.layout.Shape(\n",
    "        type=\"line\",\n",
    "        yref=\"y\",\n",
    "        xref=\"x\",\n",
    "        x0=min_alpha,\n",
    "        y0=-0.05,\n",
    "        x1=min_alpha,\n",
    "        y1=0.4,\n",
    "        line=dict(dash=\"dot\")\n",
    "    ),\n",
    "    row=3,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_type=\"log\", \n",
    "    xaxis2_type=\"log\",\n",
    "    xaxis3_type=\"log\",\n",
    "    xaxis3_title=\"Regularization Parameter\",\n",
    "    yaxis_title=\"Lasso coefficients\",\n",
    "    yaxis2_title=\"Validation Sets MSE\",\n",
    "    yaxis3_title=\"Model Performance (R²)\",\n",
    "    height=800\n",
    ")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashed line across all plots represents the best model obtained through 10-fold cross-validation. For this optimal model, the lasso regression mainly includes:\n",
    "\n",
    "- the `alcohol` and `sulphates` features, contributing positively to the `quality`;\n",
    "- the `volatile acidity` feature contributing negatively to the `quality`.\n",
    "\n",
    "This result is very close to the significant features found in the OLS model. Unexpectedly, the performance of the optimal lasso model is similar to the performance of the previous OLS model. Moreover, the benefit of cross-validation on this task seems negligible, considering the flatness of the average MSE. We can once again conclude that a linear model isn't well suited to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D'. Next Steps\n",
    "\n",
    "Since the previous linear models were not able to accurately predict the `quality` of red wines, we are interested in knowing if a new model that takes into account the previous observations could do better. Since the `quality` is almost binary, as seen on the first histogram, we fix an arbitrary threshold that will discriminate between *good* and *bad* wines. This threshold is fixed at a `quality` of 5.5, so that wines with a score of 5 or lower will be rated as *bad*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep copy used so that the original dataset isn't modified.\n",
    "red_wine_bin = red_wine_clean.copy(deep=True)\n",
    "\n",
    "# Binarize the quality feature into \"good\" and \"bad\" wines.\n",
    "bins = (2, 5.5, 8)\n",
    "group_names = [\"bad\", \"good\"]\n",
    "red_wine_bin[\"quality\"] = pd.cut(red_wine_bin.quality, bins=bins, labels=group_names)\n",
    "\n",
    "# Bar plot of wine quality.\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=[\"bad\", \"good\"],\n",
    "    y=red_wine_bin.quality.value_counts()\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Red Wine Quality – Binarized\",\n",
    "    xaxis_title=\"Quality\",\n",
    "    yaxis_title=\"Count\",\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the `quality` has been binarized, the dataset is split into training and testing sets, with a more common 80%/20% ratio. The input features are standardized to get optimal results. A linear SVM model is used for classification, because of its ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    red_wine_bin.drop(\"quality\", axis=1), \n",
    "    red_wine_bin.quality, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standardize input features.\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "clf = LinearSVC(max_iter=2000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Classification report on the testing test:\\n\")\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a mean precision and recall on the testing set of about 72%. This performance could be slightly improved by using cross-validation and a non-linear SVM (kernel trick), but other methods (random forest, k-NN, etc.) should probably show better performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Classification\n",
    "\n",
    "In this section, we will try to classify wine color from their physicochemical properties, using datasets `winequality-red` and `winequality-white`. The white wine dataset is cleaned out of outliers using the same criterion as before (outliers outside the ±3 standard deviations range are removed) and merged with the clean red wine dataset. A `color` feature is added to the dataset to distinguish red from white wines. The `quality` feature is removed as it won't be used by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read white wine dataset and apply the same cleaning procedure as before.\n",
    "white_wine = pd.read_csv(\"../input/winequality/winequality-white.csv\", sep=\";\")\n",
    "white_wine_clean = white_wine[(np.abs(stats.zscore(white_wine)) < 3).all(axis=1)]\n",
    "\n",
    "# Add color feature and merge datasets.\n",
    "wine = pd.concat([red_wine_clean.assign(color=\"red\"), white_wine_clean.assign(color=\"white\")]).drop(\"quality\", axis=1)\n",
    "\n",
    "# Plot histograms for each variable.\n",
    "fig = make_subplots(rows=4, cols=3)\n",
    "\n",
    "for i, col in enumerate(wine.columns, 1):\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=wine[col],\n",
    "            name=col\n",
    "        ), \n",
    "        row=int(np.ceil(i / 3)), \n",
    "        col=i % 3 if i % 3 != 0 else 3\n",
    "    )\n",
    "    \n",
    "fig.update_layout(\n",
    "    title_text=\"Histograms Of Red and White Wine Features\",\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both datasets merged, we observe that some histograms are bimodal, which could indicate that these features will enable discrimination of red and white wines. We can then display a scatter plot using two of these features to see if wine color can be distinguished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot chlorides vs. total sulfur dioxide.\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=wine.loc[wine.color == \"white\", \"total sulfur dioxide\"],\n",
    "    y=wine.loc[wine.color == \"white\", \"chlorides\"],\n",
    "    mode=\"markers\",\n",
    "    name=\"White Wines\"\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=wine.loc[wine.color == \"red\", \"total sulfur dioxide\"],\n",
    "    y=wine.loc[wine.color == \"red\", \"chlorides\"],\n",
    "    mode=\"markers\",\n",
    "    name=\"Red Wines\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Chlorides vs. Total Sulfur Dioxide\",\n",
    "    xaxis_title=\"Total Sulfur Dioxide\",\n",
    "    yaxis_title=\"Chlorides\",\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, using only two features, wines can be almost perfectly visually classified according to their color. Given its ease of use compared to a PCA + k-mean clustering classifier, a linear SVM model is used once again. Considering the observations on the histograms, the dataset is split into training and testing sets using a 10%/90% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    wine.drop(\"color\", axis=1), \n",
    "    wine.color, \n",
    "    test_size=0.90, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standardize input features.\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Fit linear SVM and print accuracy.\n",
    "clf = LinearSVC(max_iter=2000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mean accuracy on training set: {:.2f}%\".format(100*clf.score(X_train, y_train)))\n",
    "print(\"Mean accuracy on testing set: {:.2f}%\".format(100*clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the linear SVM classifier is perfectly suited for this task: the error on the testing set is only about 0.2%. Finally, we can study which variables should be kept to keep this error below 1%. The `chlorides`, `total sulfur dioxide`, `density` and `residual sugar` features are selected based on their histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets.\n",
    "wine_test = wine[[\"chlorides\", \"total sulfur dioxide\", \"density\", \"residual sugar\", \"color\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    wine_test.drop(\"color\", axis=1), \n",
    "    wine_test.color, \n",
    "    test_size=0.90, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standardize input features.\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Fit linear SVM and print accuracy.\n",
    "clf = LinearSVC(max_iter=2000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mean accuracy on training set: {:.2f}%\".format(100*clf.score(X_train, y_train)))\n",
    "print(\"Mean accuracy on testing set: {:.2f}%\".format(100*clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, keeping only these features, the test error is only about 0.7%: we were able to reduce the dataset to only four input features. Therefore, measuring only these four physicochemical properties should be sufficient to achieve a good color classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Clustering Of Handwritten Digits\n",
    "\n",
    "In this section, we consider the Handwritten Digits Data Set (E. Alpaydin *et al.*, 1998), available from https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/, and in particular the `optdigits.tes` testing dataset.\n",
    "\n",
    "The dataset was obtained by processing 32x32 normalized bitmaps of handwritten digits. These bitmaps were divided into nonoverlapping 4x4 blocks and the number of *on* pixels were counted in each block. This operation reduced each bitmap to an 8x8 input matrix where each element is an integer in the range [0, 16]. Each row of the `optidigits.tes` dataset contains an 8x8 matrix reshaped to a horizontal 1x64 vector, with an additional class attribute column specifying the digit.\n",
    "\n",
    "The goal of this section is to cluster the data without taking into account the labels, thus studying an unsupervised learning method. The class attribute column containing the label will be used to assess the results.\n",
    "\n",
    "The data is read and the 64 input features are standardized to zero mean and unit variance. As we can see below, even with the dimension reduction applied by the authors, most digits are still identifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and separate input features and labels.\n",
    "dig = np.loadtxt(\"../input/optidigits/optdigits.tes\", dtype='i', delimiter=',')\n",
    "X = dig[:, :-1]\n",
    "y = dig[:, -1]\n",
    "\n",
    "n, m = X.shape\n",
    "\n",
    "# Plot some examples from the dataset.\n",
    "fig = make_subplots(rows=3, cols=3)\n",
    "\n",
    "for i in range(1,10):\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=np.flip(X[i, :].reshape((8, 8)), 0),\n",
    "            showscale=False,\n",
    "            colorscale=\"gray\",\n",
    "            name=\"Label: {}\".format(y[i])\n",
    "        ),\n",
    "        row=int(np.ceil(i / 3)), \n",
    "        col=i % 3 if i % 3 != 0 else 3\n",
    "    )\n",
    "    \n",
    "fig.update_layout(\n",
    "    title=\"Handwritten Digits Dataset, Examples\",\n",
    "    height=800, \n",
    "    width=800\n",
    ")\n",
    "    \n",
    "fig.show()\n",
    "\n",
    "# Standardize input features.\n",
    "X_norm = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. K-means Clustering\n",
    "\n",
    "In this section, a K-means clustering method will be applied to cluster the data. First, we would like to find a good 2D visualization of the dataset. The projection of the data on the top two principal components is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA and plot the projected input features on top 2 components.\n",
    "pca_model = PCA()\n",
    "pca_result = pca_model.fit_transform(X_norm)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(10):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pca_result[y==i, 0],\n",
    "        y=pca_result[y==i, 1],\n",
    "        mode=\"markers\",\n",
    "        marker_color=i,\n",
    "        name=\"True Label: {}\".format(i)\n",
    "    ))\n",
    "    \n",
    "fig.update_layout(\n",
    "    title=\"PCA Visualization Of Handwritten Digits\",\n",
    "    xaxis_title=\"PCA 1 ({:.2f}%)\".format(pca_model.explained_variance_ratio_[0]*100),\n",
    "    yaxis_title=\"PCA 2 ({:.2f}%)\".format(pca_model.explained_variance_ratio_[1]*100),\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, individual clusters are hardly identifiable on this visualization and the top two components explain a very low variance ratio. The Handwritten Digits dataset being a non-trivial high-dimensional structure, this type of linear projections won't be able to perform well. Luckily, t-SNE, a popular visualization algorithm that is often very succesful at revealing clusters in data, can be used in this situation. t-SNE roughly works by trying to optimize for preserving the topology of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit t-SNE with PCA initialization.\n",
    "tsne = TSNE(n_components=2, init=\"pca\", random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_norm)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(10):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_tsne[y==i, 0],\n",
    "        y=X_tsne[y==i, 1],\n",
    "        mode=\"markers\",\n",
    "        marker_color=i,\n",
    "        name=\"True Label: {}\".format(i)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE Visualization Of Handwritten Digits\",\n",
    "    height=800\n",
    ")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, t-SNE does an impressive job at finding clusters compared to PCA. However, it is still prone to getting stuck in local minima, as is the case for some images of ones. \n",
    "\n",
    "We can now perform K-means clustering and visualize its performance using t-SNE compared to the above plot. The clustering will be performed using K=10 clusters (for the 10 available digits) and the results of 10 different initial conditions will be compared. These initial conditions should not be chosen based on the t-SNE results, since neither distances nor density are preserved well (cf. *Intrinsic t-Stochastic Neighbor Embedding for Visualization and Outlier Detection – A Remedy Against the Curse of Dimensionality?*). Therefore, initial cluster centroids will be selected at random from the observations.\n",
    "\n",
    "To evaluate the clustering performance, two criterion will be compared:\n",
    "\n",
    "- the intertia, corresponding to the sum of distances of samples to their closest cluster center;\n",
    "- an arbitrary score, corresponding to the fraction of pairs of samples that are correctly partitioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store scores as defined by the exercise, and k-means inertias.\n",
    "scores = []\n",
    "inertias = []\n",
    "for cond in range(10):\n",
    "    # Perform k-means, using a fixed random state for reproducibility.\n",
    "    kmeans = KMeans(n_clusters=10, init=\"random\", n_init=1, random_state=cond).fit(X_norm)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Compute clustering score.\n",
    "    digit_pairs = np.zeros((n, n))\n",
    "    for i in range(0, n):\n",
    "        for j in range(i+1, n):\n",
    "            digit_pairs[i,j] = (labels[i]==labels[j] and y[i]== y[j]) or (labels[i]!=labels[j] and y[i]!=y[j])\n",
    "    scores.append(100*np.sum(digit_pairs)*2/(n*(n-1))) \n",
    "    inertias.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum score: {:.2f}%\".format(np.min(scores)))\n",
    "print(\"Maximum score: {:.2f}%\".format(np.max(scores)))\n",
    "\n",
    "# Plot normalized criteria.\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(range(1, 10)),\n",
    "    y=(scores - np.mean(scores)) / np.std(scores),\n",
    "    name=\"Clustering Score\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(range(1, 10)),\n",
    "    y=(inertias - np.mean(inertias)) / np.std(inertias),\n",
    "    name=\"Clustering Inertia\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Normalized Clustering Score And Inertia\",\n",
    "    xaxis_title=\"Condition Number\",\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the bar plot above, the clustering score and inertia seem to be uncorrelated. The maximum score reached is about 93% and doesn't seem to vary much between conditions. Below is displayed the result of a K-means clustering with K=10 and using the k-means++ algorithm with 10 repetitions for finding initial centroids (D. Arthur *et al.*, 2007)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means using 10 clusters, k-means++ and 10 repetitions.\n",
    "kmeans = KMeans(n_clusters=10, random_state=42).fit(X_norm)\n",
    "\n",
    "# Plot t-SNE visualization with infered labels.\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(10):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_tsne[kmeans.labels_==i, 0],\n",
    "        y=X_tsne[kmeans.labels_==i, 1],\n",
    "        mode=\"markers\",\n",
    "        marker_color=i,\n",
    "        name=\"Infered Label: {}\".format(i)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE Visualization Of Handwritten Digits, K-means clustering\",\n",
    "    height=800\n",
    ")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing with the first t-SNE visualization, the K-means clustering was able to correctly identify some of the clusters, but still fails on most of them. For example, the ones are mostly absorbed in the original cluster of eights, while the cluster of threes, fives and nines are merged into one cluster.\n",
    "\n",
    "Considering that the correct number of clusters was known beforehand, we are now interested in knowing if this parameter can be infered from the data. The performance and inertia of K-means clusterings with K varying from 5 to 20 is compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store custom score and k-means inertias.\n",
    "scores = []\n",
    "inertias = []\n",
    "for n_clusters in range(5, 21):\n",
    "    # Perform k-means, using a fixed random state for reproducibility.\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=n_clusters).fit(X_norm)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Compute clustering score.\n",
    "    digit_pairs = np.zeros((n, n))\n",
    "    for i in range(0, n):\n",
    "        for j in range(i+1,n):\n",
    "            digit_pairs[i,j] = (labels[i]==labels[j] and y[i]== y[j]) or (labels[i]!=labels[j] and y[i]!=y[j])\n",
    "    scores.append(100*np.sum(digit_pairs)*2/(n*(n-1))) \n",
    "    inertias.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum score: {:.2f}%\".format(np.min(scores)))\n",
    "print(\"Maximum score: {:.2f}%\".format(np.max(scores)))\n",
    "\n",
    "# Plot normalized criteria.\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(range(5, 21)),\n",
    "    y=(scores - np.mean(scores)) / np.std(scores),\n",
    "    name=\"Clustering Score\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(range(5, 21)),\n",
    "    y=(inertias - np.mean(inertias)) / np.std(inertias),\n",
    "    name=\"Clustering Inertia\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Normalized Clustering Score And Inertia\",\n",
    "    xaxis_title=\"Condition Number\",\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from the structure complexity of this dataset, simply looking at these criteria doesn't give any clue on the correct number of clusters that one should use. When K varies from 5 to 20, the inertia decreases: this is expected, since adding more centroids will decrease the sum of distances of samples to their closest cluster center. On the other hand, the clustering score increases way past the K = 10 mark, without giving any clue on the correct number of clusters.\n",
    "\n",
    "Finally, we observed that K-means clustering alone wasn't able to accurately find meaningful clusters or subclusters in the data. To improve its performance, it should be applied after a dimensionality reduction step such as PCA or MDS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Hierarchical Clustering\n",
    "\n",
    "In this section, we will study a new clustering method: agglomerative hierarchical clustering. In particular, we are interested in the effect of different linkage methods on the resulting clusters:\n",
    "\n",
    "- **ward** minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach;\n",
    "- **maximum or complete linkage** minimizes the maximum distance between observations of pairs of clusters;\n",
    "- **average linkage** minimizes the average of the distances between all observations of pairs of clusters;\n",
    "- **single linkage** minimizes the distance between the closest observations of pairs of clusters.\n",
    "\n",
    "25 samples of each of the first three digits will be taken at random and the clustering results will be visualized using dendrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 25 samples of each of the first three digits.\n",
    "n_samples = 25\n",
    "n_digits = 3\n",
    "samples = np.zeros((n_digits, n_samples, m))\n",
    "for i in range(n_digits):\n",
    "    idx = np.random.choice(np.where(y == i)[0], size=n_samples, replace=False)\n",
    "    samples[i] = X[idx]\n",
    "\n",
    "# Reshape samples into a single array.\n",
    "data_array = np.reshape(samples, (n_digits*n_samples, m))\n",
    "labels = [0]*25 + [1]*25 + [2]*25\n",
    "\n",
    "# Plot dendrogram for ward linkage.\n",
    "fig1 = ff.create_dendrogram(\n",
    "    data_array,\n",
    "    labels=labels,\n",
    "    linkagefun=lambda x: linkage(data_array, method=\"ward\", metric=\"euclidean\"),\n",
    "    orientation=\"bottom\",\n",
    "    color_threshold=120\n",
    ")\n",
    "\n",
    "fig1.update_layout(title=\"Ward Linkage Dendrogram\",\n",
    "    xaxis_title=\"True Digit\",\n",
    "    yaxis_title=\"Distance Between Children\"\n",
    ")\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram for average linkage.\n",
    "fig3 = ff.create_dendrogram(\n",
    "    data_array,\n",
    "    labels=labels,\n",
    "    linkagefun=lambda x: linkage(data_array, method=\"average\", metric=\"euclidean\"),\n",
    "    orientation=\"bottom\",\n",
    "    color_threshold=47\n",
    ")\n",
    "\n",
    "fig3.update_layout(title=\"Average Linkage Dendrogram\",\n",
    "    xaxis_title=\"True Digit\",\n",
    "    yaxis_title=\"Distance Between Children\"\n",
    ")\n",
    "\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram for complete linkage.\n",
    "fig4 = ff.create_dendrogram(\n",
    "    data_array,\n",
    "    labels=labels,\n",
    "    linkagefun=lambda x: linkage(data_array, method=\"complete\", metric=\"euclidean\"),\n",
    "    orientation=\"bottom\",\n",
    "    color_threshold=60\n",
    ")\n",
    "\n",
    "fig4.update_layout(title=\"Complete Linkage Dendrogram\",\n",
    "    xaxis_title=\"True Digit\",\n",
    "    yaxis_title=\"Distance Between Children\"\n",
    ")\n",
    "\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram for single linkage.\n",
    "fig2 = ff.create_dendrogram(\n",
    "    data_array,\n",
    "    labels=labels,\n",
    "    linkagefun=lambda x: linkage(data_array, method=\"single\", metric=\"euclidean\"),\n",
    "    orientation=\"bottom\",\n",
    "    color_threshold=32\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title=\"Single Linkage Dendrogram\",\n",
    "    xaxis_title=\"True Digit\",\n",
    "    yaxis_title=\"Distance Between Children\"\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color thresholds were fixed to identify meaningful clusters. We observe on the dendrograms that for all linkage methods, the zeros are almost perfectly clustered, in particular using single and ward linkage. However, the ones and twos show more overlap: only the ward linkage shows almost perfect clustering, while the average linkage displays minimal overlap. Single and complete linkage aren't able to cluster these digits in a meaningful way. This overlap between ones and twos is consistent with the mis-grouping observed on the t-SNE visualization: this could indicate that there is some interpenetration of these digits in the raw dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
